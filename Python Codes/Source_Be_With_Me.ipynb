{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de125f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"16\"  # Set the number of inter-op threads\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"16\"  # Set the number of intra-op threads\n",
    "\n",
    "np.random.seed(903949505)\n",
    "pd.set_option('display.precision', 2)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cee023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4df9c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda(batmobile):\n",
    "    gc.collect()\n",
    "    print(color.BOLD + color.UNDERLINE + color.GREEN + \"Shape\" + color.END)\n",
    "    print(\"Number of columns are \" + color.BOLD + str(batmobile.shape[0]) + color.END + \" and number of rows are \" + color.BOLD + str(batmobile.shape[1]) + color.END + \"\\n\")\n",
    "    print(color.BOLD + color.UNDERLINE + color.PURPLE + \"Data types\" + color.END)\n",
    "    display(batmobile.dtypes)\n",
    "    print(color.BOLD + color.UNDERLINE + color.DARKCYAN + \"Sample rows - Top 5\" + color.END)\n",
    "    display(batmobile.head())\n",
    "    print(\"\\n\" + \"\\n\" + color.BOLD + color.UNDERLINE + color.WARNING + \"EDA Statistics\" + color.END)\n",
    "    display(batmobile.describe())\n",
    "    nan_count = batmobile.isna().sum()\n",
    "    print(\"\\n\" + \"\\n\" + color.BOLD + color.UNDERLINE + color.RED + \"Missing values\" + color.END)\n",
    "    display(nan_count[nan_count > 0])\n",
    "    print(\"\\n\" + \"\\n\" + color.BOLD + color.UNDERLINE + color.BLUE + \"Count of Outliers\" + color.END)\n",
    "    Q1 = batmobile.quantile(0.25)\n",
    "    Q3 = batmobile.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    Outliers = ((batmobile < (Q1 - 3 * IQR)) | (batmobile > (Q3 + 3 * IQR))).sum()\n",
    "    display(Outliers[Outliers>0])\n",
    "    #check for null/missing values\n",
    "    batmobile.info()\n",
    "    msno.bar(batmobile)\n",
    "    sns.set(rc={'figure.figsize':(15,10)})\n",
    "    sns.heatmap(batmobile.iloc[:,:-1].corr(), annot=True, cmap=\"YlGnBu\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "398597ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train and test dataset using sklearn\n",
    "def train_test(vegito):\n",
    "    goku, vegita = train_test_split(vegito, test_size=0.2, random_state=903949505)\n",
    "    return goku, vegita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "607c1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stratified sampling in sklearn\n",
    "def stratified_sampling(vegito, target, size = 0.4, seed = 903949505):\n",
    "    gc.collect()\n",
    "    goku, vegita = train_test_split(vegito, test_size=size, stratify=vegito[target], random_state=seed)\n",
    "    print(color.BOLD + color.UNDERLINE + color.DARKCYAN + \"Original distribution\" + color.END)\n",
    "    c = vegito[target].value_counts(normalize=False)\n",
    "    p = vegito[target].value_counts(normalize=True)*100\n",
    "    display(pd.concat([c,p], axis=1, keys=['counts', '%']))\n",
    "    print(\"\\n\" + \"\\n\" + color.BOLD + color.UNDERLINE + color.WARNING + \"Train distribution\" + color.END)\n",
    "    c = goku[target].value_counts(normalize=False)\n",
    "    p = goku[target].value_counts(normalize=True)*100\n",
    "    display(pd.concat([c,p], axis=1, keys=['counts', '%']))\n",
    "    print(\"\\n\" + \"\\n\" + color.BOLD + color.UNDERLINE + color.RED + \"Test distribution\" + color.END)\n",
    "    c = vegita[target].value_counts(normalize=False)\n",
    "    p = vegita[target].value_counts(normalize=True)*100\n",
    "    display(pd.concat([c,p], axis=1, keys=['counts', '%']))\n",
    "    \n",
    "    #split goku, vegita and cell by target variable to make depent and indepent data seperately\n",
    "    goku_X = goku.drop(target, axis=1)\n",
    "    vegita_X = vegita.drop(target, axis=1)\n",
    "    goku_y = pd.DataFrame(goku[target])\n",
    "    vegita_y = pd.DataFrame(vegita[target])\n",
    "    \n",
    "    #return goku_X, goku_y, vegita_X, vegita_y\n",
    "    print(\"\\n\" + \"\\n\" + color.BOLD + color.UNDERLINE + color.BLUE + \"Train_x shape\" + color.END)\n",
    "    print(\"Number of columns are \" + color.BOLD + str(goku_X.shape[0]) + color.END + \" and number of rows are \" + color.BOLD + str(goku_X.shape[1]) + color.END)\n",
    "    display(goku_X.head())\n",
    "    print(\"\\n\" + \"\\n\" + color.BOLD + color.UNDERLINE + color.BLUE + \"Train_y shape\" + color.END)\n",
    "    print(\"Number of columns are \" + color.BOLD + str(goku_y.shape[0]) + color.END + \" and number of rows are \" + color.BOLD + str(goku_y.shape[1]) + color.END)\n",
    "    display(goku_y.head())\n",
    "    print(\"\\n\" + \"\\n\" + color.BOLD + color.UNDERLINE + color.BLUE + \"Train_x shape\" + color.END)\n",
    "    print(\"Number of columns are \" + color.BOLD + str(vegita_X.shape[0]) + color.END + \" and number of rows are \" + color.BOLD + str(vegita_X.shape[1]) + color.END)\n",
    "    display(vegita_X.head())\n",
    "    print(\"\\n\" + \"\\n\" + color.BOLD + color.UNDERLINE + color.BLUE + \"Train_y shape\" + color.END)\n",
    "    print(\"Number of columns are \" + color.BOLD + str(vegita_y.shape[0]) + color.END + \" and number of rows are \" + color.BOLD + str(vegita_y.shape[1]) + color.END)\n",
    "    display(vegita_y.head())\n",
    "    return goku_X, goku_y, vegita_X, vegita_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4cccfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send train and test data in sklearn decision tree model with min_samples_leaf, and min_samples_split \n",
    "def decision_tree(X, Y, criterion= 'gini', max_depth=None, min_samples_leaf=1, min_samples_split=2, random_state=903949505):\n",
    "    gc.collect()\n",
    "    model = DecisionTreeClassifier(random_state=random_state, max_depth=max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split)\n",
    "    model.fit(X, Y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24d22cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict using decision tree from sklearn and calculate accuracy and F1 score\n",
    "def predict(model, X, Y):\n",
    "    gc.collect()\n",
    "    predictions = model.predict(X)\n",
    "    predictions = pd.DataFrame(predictions, index=X.index)\n",
    "    accuracy = accuracy_score(Y, predictions)\n",
    "    f1 = f1_score(Y, predictions)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    return predictions, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9033acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build knn on X and Y with distnace and n_neighbors\n",
    "def knn(X, Y, distance='euclidean', n_neighbors=5):\n",
    "    gc.collect()\n",
    "    model = KNeighborsClassifier(n_neighbors=n_neighbors, metric=distance)\n",
    "    model.fit(X, Y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0685bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a gradient boosting from sklearn\n",
    "def gradient_boosting(X, Y, learning_rate=0.1, n_estimators=10000, subsample=1.0, max_depth=6):\n",
    "    gc.collect()\n",
    "    model = GradientBoostingClassifier(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth)\n",
    "    model.fit(X, Y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fed8dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost(X, Y, learning_rate=0.1, n_estimators=10000, subsample=1.0, max_depth=6):\n",
    "    gc.collect()\n",
    "    xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=903949505, learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth, nthread=-1)\n",
    "    xgb_model.fit(X, Y)\n",
    "    return xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "454b6399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnet(X, Y, learning_rate=0.001, loss='binary_crossentropy', epochs=10, batch_size=32, validation_split=0.2, patience=15):\n",
    "    gc.collect()\n",
    "    NN = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X.shape[1],)),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    NN.compile(optimizer=Adam(learning_rate=learning_rate), loss=loss, metrics=[AUC()])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "    NN.fit(X, Y, epochs=epochs, batch_size=batch_size, validation_split=validation_split, callbacks=[early_stopping])\n",
    "    return NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1516ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curve_self(h1, X_train, y_train, classifier, title = \"Learning Curve (Decision Tree)\", train_sizes=np.linspace(0.001, 0.04, 10), cv=5):\n",
    "    # Choose Decision Tree classifier\n",
    "    gc.collect()\n",
    "    header = 24\n",
    "    other_title = 18\n",
    "\n",
    "    # Define the sample sizes you want to use for the learning curve\n",
    "    f1_scorer = make_scorer(f1_score)\n",
    "    train_sizes, train_scores, test_scores = learning_curve(classifier, X_train, y_train, train_sizes=train_sizes, scoring='accuracy', cv=cv, n_jobs=-1, verbose=3, random_state=903949505)\n",
    "\n",
    "    # Calculate the mean and standard deviation of training and testing scores\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot the learning curve\n",
    "    plt.clf()\n",
    "    plt.gcf().set_facecolor('white')\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_facecolor('white')\n",
    "    ax.set_facecolor('white')\n",
    "    plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Training Accuracy', alpha=0.8)\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "\n",
    "    plt.plot(train_sizes, test_mean, color='green', marker='s', markersize=5, label='Testing Accuracy', alpha=0.8)\n",
    "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='green')\n",
    "\n",
    "    plt.xlabel('Number of Training Samples', fontsize=other_title)\n",
    "    plt.ylabel('Accuracy', fontsize=other_title)\n",
    "    plt.title(title, fontsize=header)\n",
    "    plt.legend(loc='lower right', frameon=True, edgecolor='black', facecolor='white', fontsize=other_title)\n",
    "    plt.tick_params(direction='in', bottom=True, top=True, left=True, right=True, length=4)\n",
    "    plt.gca().spines['top'].set_color('black')\n",
    "    plt.gca().spines['bottom'].set_color('black')\n",
    "    plt.gca().spines['left'].set_color('black')\n",
    "    plt.gca().spines['right'].set_color('black')\n",
    "    spine_alpha = 0.5\n",
    "    plt.gca().spines['top'].set_alpha(spine_alpha)\n",
    "    plt.gca().spines['bottom'].set_alpha(spine_alpha)\n",
    "    plt.gca().spines['left'].set_alpha(spine_alpha)\n",
    "    plt.gca().spines['right'].set_alpha(spine_alpha)\n",
    "    plt.grid(True, linestyle=\"dotted\", alpha=0.45, color='black')\n",
    "    plt.xticks(fontsize=other_title)\n",
    "    plt.yticks(fontsize=other_title)\n",
    "    ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: '{:,.0f}'.format(x)))\n",
    "    plt.savefig(h1)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a8df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(Xtrain, Xtest, Ytrain, Ytest, param_grid, col1, col2, classifier,\n",
    "                excelpath,\n",
    "                printpath, title, form, custom_x_values,\n",
    "                Change_name=False, new_labels=['A', 'B', 'C', 'D', 'E'],\n",
    "                change_ylim = False, ylim_lower = 0, ylim_upper = 1, data_type_param = float):\n",
    "    gc.collect()\n",
    "    header = 24\n",
    "    other_title = 18\n",
    "    \n",
    "    # Perform Grid Search with 5-fold cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        classifier,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        return_train_score=True,\n",
    "        verbose=3\n",
    "    )\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    grid_search.fit(Xtrain, Ytrain)\n",
    "\n",
    "    # Extract relevant information from the results dataframe\n",
    "    results_summary = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    # Extract relevant columns for the summary\n",
    "    columns_to_keep = [col1, 'mean_train_score', 'mean_test_score', 'std_train_score', 'std_test_score', 'mean_fit_time', 'mean_score_time']\n",
    "    results_summary = results_summary[columns_to_keep]\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    results_summary.columns = [col2, 'Train Score', 'Test Score', 'Train Score(std)', 'Test Score(std)', 'Train Time', 'Test Time']\n",
    "\n",
    "    # Plot the validation curve\n",
    "    plt.clf()\n",
    "    plt.gcf().set_facecolor('white')\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_facecolor('white')\n",
    "    ax.set_facecolor('white')\n",
    "    \n",
    "    results_summary['diff']=pd.to_numeric(results_summary['Train Score'] - results_summary['Train Score(std)'])\n",
    "    results_summary['sum']=pd.to_numeric(results_summary['Train Score'] + results_summary['Train Score(std)'])\n",
    "    \n",
    "    plt.plot(results_summary[col2], results_summary['Train Score'], color='blue', marker='o', markersize=5, label='Training Accuracy', alpha=0.8)\n",
    "    if data_type_param == float:\n",
    "        plt.fill_between(results_summary[col2].values.astype(float), results_summary['diff'].values.astype(float), results_summary['sum'].values.astype(float), alpha=0.1, color='blue')\n",
    "    else:\n",
    "        plt.fill_between(results_summary[col2], results_summary['diff'].values.astype(int), results_summary['sum'].values.astype(int), alpha=0.1, color='blue')\n",
    "    results_summary['diff']=pd.to_numeric(results_summary['Test Score'] - results_summary['Test Score(std)'])\n",
    "    results_summary['sum']=pd.to_numeric(results_summary['Test Score'] + results_summary['Test Score(std)'])\n",
    "    \n",
    "    plt.plot(results_summary[col2], results_summary['Test Score'], color='green', marker='s', markersize=5, label='Testing Accuracy', alpha=0.8)\n",
    "    if data_type_param == float:\n",
    "        plt.fill_between(results_summary[col2].values.astype(float), results_summary['diff'].values.astype(float), results_summary['sum'].values.astype(float), alpha=0.1, color='green')\n",
    "    else:\n",
    "        plt.fill_between(results_summary[col2], results_summary['diff'].values.astype(float), results_summary['sum'].values.astype(float), alpha=0.1, color='green')\n",
    "        \n",
    "    plt.xlabel(col2, fontsize=other_title)\n",
    "    plt.ylabel('Accuracy', fontsize=other_title)\n",
    "    plt.title(title, fontsize=header)\n",
    "    plt.legend(loc='lower right', frameon=True, edgecolor='black', facecolor='white', fontsize=other_title)\n",
    "    plt.tick_params(direction='in', bottom=True, top=True, left=True, right=True, length=4)\n",
    "    plt.gca().spines['top'].set_color('black')\n",
    "    plt.gca().spines['bottom'].set_color('black')\n",
    "    plt.gca().spines['left'].set_color('black')\n",
    "    plt.gca().spines['right'].set_color('black')\n",
    "    spine_alpha = 0.5\n",
    "    plt.gca().spines['top'].set_alpha(spine_alpha)\n",
    "    plt.gca().spines['bottom'].set_alpha(spine_alpha)\n",
    "    plt.gca().spines['left'].set_alpha(spine_alpha)\n",
    "    plt.gca().spines['right'].set_alpha(spine_alpha)\n",
    "    plt.grid(True, linestyle=\"dotted\", alpha=0.45, color='black')\n",
    "    plt.xticks(fontsize=other_title)\n",
    "    plt.yticks(fontsize=other_title)\n",
    "    ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: form.format(x)))\n",
    "    plt.xticks(custom_x_values)\n",
    "    if Change_name:\n",
    "        ax.set_xticklabels(labels=new_labels)\n",
    "    if change_ylim:\n",
    "        plt.ylim(ylim_lower, ylim_upper)\n",
    "    plt.savefig(printpath)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.show()\n",
    "    \n",
    "    columns_to_keep = [col2, 'Train Score', 'Test Score', 'Train Time', 'Test Time']\n",
    "    results_summary = results_summary[columns_to_keep]\n",
    "    results_summary[['Train Score', 'Test Score', 'Train Time', 'Test Time']] = results_summary[['Train Score', 'Test Score', 'Train Time', 'Test Time']].round(2)\n",
    "    results_summary.to_excel(excelpath, index=False)\n",
    "    \n",
    "    return results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e139b8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
